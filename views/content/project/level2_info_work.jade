include ./mixins.jade

div 
  | As we’ve seen throughout the history, technological progress and automation of human labor simply changed the nature of work and oftentimes created more new jobs. However, since the future is a place of accelerating changes, it would be unwise trying to predict it by only looking at the past. In other words, if new jobs simply appeared in the past, it doesn’t mean they always will, at least in traditional sense of the notion.
div 
  | World Economic Forum report on Future of Jobs 
  +link('http://www3.weforum.org/docs/WEF_Future_of_Jobs.pdf')
  | estimates the creation of 2 million new jobs alongside the elimination of 7 million by 2020. Which means a net loss of 5 million jobs. Another frequently cited study by Oxford Martin School 
  +link('https://www.technologyreview.com/s/519241/report-suggests-nearly-half-of-us-jobs-are-vulnerable-to-computerization/')
  | estimates the automation of about half of the jobs existing today by 2033. Lastly, now even the White House, in the report to Congress 
  +link('https://www.whitehouse.gov/sites/default/files/docs/ERP_2016_Book_Complete%20JA.pdf')
  | , estimates 83 percent chances of eventually losing job to a machine for a worker making $20 an hour or less. Even highly paid workers earning as much as $40 an hour face chances of 31 percent. This is all is being achieved thanks to machine learning, or artificial intelligence, that has an impact on any industry in any country: from automatic language translation to self driving vehicles to optimization of processes in production, marketing and advertising. In other words, machine learning have the capability of drastically impacting all economies — by eliminating millions of jobs within a short span of time.
//- +pic('1_lotsofdata.png')
div
  | When it comes to information, we are creating increasingly more of it every day. Thanks to digitization of our daily activities and further proliferation of Internet, where in 2015 alone, we are liking 4.2 million things on Facebook, uploading 300 hours of video on YouTube and creating 350,000 tweets every single minute 
  +link('https://www.domo.com/blog/2015/08/data-never-sleeps-3-0/')
  | , the amount of data we are creating is doubling every 1.5 years 
  +link('http://www.datamation.com/applications/big-data-analytics-overview.html')
  | . According to 2013 report by SINTEF, 90% of all information in the world has been created in the prior two years 
  +link('https://www.sciencedaily.com/releases/2013/05/130522085217.htm')
  | . Everything we do is generating data like never before, and lots of data is exactly what machines need in order to learn.
div
  | Imagine programming a computer to recognize a table. You would need to write a lot of instructions and complex algorithms, and still you would have a program that would never do it as well as a human being. However, if instead of providing computer complex algorithms, you upload thousands of images of things that are similar to tables — it would do a much better job. This is called reinforcement learning, and is very similar to the process human beings use to learn. When we learn to recognize a table, the label “table” gets connected to every table we see, such that certain neural pathways are reinforced and others aren’t. For “table” to fire in our brains, what we see has to be close enough to our previous table encounters. This principle is clearly visible in the 
  +exp('languageHistory', 3, 'history of automatic language translation', '.')
    div
      | With the first developments of machine learning, or Artificial Intelligence, people have tried to create an algorithm that would translate between languages. Initially this sounded like a comparatively simple task: in 1960s Marvin Minsky, pioneer in the field of Artificial Intelligence, then professor at MIT AI Lab, tasked his students with developing such an algorithm as part of a summer project. This has proved to be unsuccessful, however. Creating an algorithm that would account for all the nuances and subtleties of human speech proved to be too difficult. It took AI enthusiasts 30 more years of trials before people at IBM found the working formula. The secret was simple: to use multiple examples of translations by real people as a material for teaching the machine. Today, all major automatic language translators work this way. Data with samples of millions of real translations is gathered across the internet, and is being fed to algorithms to perform translations online. This principle allows computer translators to remain up to date even despite natural evolution of human language and appearance of slang. 
div 
  | Currently, artificial intelligence algorithms have been developed such that machines can exceed human intelligence for completing specific tasks efficiently – ranging from automatic language translation to self driven cars
  +link('http://fortune.com/2016/02/10/google-self-driving-cars-artificial-intelligence/')
  | . Considering the pace
  +link('http://28oa9i1t08037ue3m1l0i861.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/PPTExponentialGrowthof_Computing-1.jpg')
  +link('http://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP')
  | and the trajectory of development of the artificial intelligence framework
  +link('http://www.wired.com/2010/08/reverse-engineering-brain-kurzweil/')
  | , in the future, technologies will be capable of doing everything that human beings are employed for presently, while utilising vastly less human labour. In other words, more and more jobs in the future will be automated by algorithms and machines, but in order to function, those machines need information that is still produced by people. 